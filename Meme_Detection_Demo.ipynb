{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Hateful Meme Detection - Live Demo\n",
    "### Graph-based Multimodal Fusion with CLIP + Graphormer\n",
    "\n",
    "**Instructions:**\n",
    "1. Click \"Runtime\" ‚Üí \"Run all\" (or Ctrl+F9)\n",
    "2. Wait for model to load (~2 minutes)\n",
    "3. Upload your meme image when prompted\n",
    "4. Enter the meme text\n",
    "5. Get instant prediction!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch torchvision pillow numpy scikit-learn gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model from Google Drive\n",
    "\n",
    "**Automatically loads `best_graphormer.pt` from your Google Drive:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"üìÇ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to your model in Google Drive (root folder)\n",
    "model_path = '/content/drive/MyDrive/best_graphormer.pt'\n",
    "\n",
    "# Check if model exists\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"‚úÖ Model found: {model_path}\")\n",
    "    print(f\"   Size: {os.path.getsize(model_path) / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found!\")\n",
    "    print(\"\\nPlease make sure 'best_graphormer.pt' is in your Google Drive root folder.\")\n",
    "    print(\"Or update the path above if it's in a subfolder.\")\n",
    "    raise FileNotFoundError(\"Model file not found in Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Architecture Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "# Graph builder constants\n",
    "EDGE_TEXT_TEXT = 0\n",
    "EDGE_IMG_IMG = 1\n",
    "EDGE_TEXT_IMG = 2\n",
    "EDGE_GLOBAL = 3\n",
    "\n",
    "def build_graph(text_feats, image_feats, top_k=8):\n",
    "    \"\"\"Build heterogeneous graph from text/image features.\"\"\"\n",
    "    device = text_feats.device\n",
    "    text_len = text_feats.shape[0]\n",
    "    img_len = image_feats.shape[0]\n",
    "    \n",
    "    # Compute grid size\n",
    "    grid_size = int(img_len ** 0.5)\n",
    "    assert grid_size * grid_size == img_len, f\"Image patches {img_len} must be square\"\n",
    "    \n",
    "    node_feats = torch.cat([text_feats, image_feats], dim=0)\n",
    "    global_feat = node_feats.mean(dim=0, keepdim=True)\n",
    "    x = torch.cat([node_feats, global_feat], dim=0)\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_type = []\n",
    "    edge_weight = []\n",
    "    \n",
    "    # Text-text chain\n",
    "    for i in range(max(text_len - 1, 0)):\n",
    "        edge_index.extend([(i, i + 1), (i + 1, i)])\n",
    "        edge_type.extend([EDGE_TEXT_TEXT, EDGE_TEXT_TEXT])\n",
    "        edge_weight.extend([0.0, 0.0])\n",
    "    \n",
    "    # Image-image grid\n",
    "    for p in range(img_len):\n",
    "        row, col = divmod(p, grid_size)\n",
    "        neighbors = []\n",
    "        if row > 0: neighbors.append((row - 1) * grid_size + col)\n",
    "        if row < grid_size - 1: neighbors.append((row + 1) * grid_size + col)\n",
    "        if col > 0: neighbors.append(row * grid_size + (col - 1))\n",
    "        if col < grid_size - 1: neighbors.append(row * grid_size + (col + 1))\n",
    "        \n",
    "        for n in neighbors:\n",
    "            src = text_len + p\n",
    "            dst = text_len + n\n",
    "            edge_index.append((src, dst))\n",
    "            edge_type.append(EDGE_IMG_IMG)\n",
    "            edge_weight.append(0.0)\n",
    "    \n",
    "    # Text-image edges (L2-normalized cosine similarity)\n",
    "    if text_len > 0 and img_len > 0:\n",
    "        text_norm = F.normalize(text_feats, p=2, dim=-1)\n",
    "        image_norm = F.normalize(image_feats, p=2, dim=-1)\n",
    "        sim = torch.matmul(text_norm, image_norm.t())\n",
    "        \n",
    "        k = min(top_k, img_len)\n",
    "        topk_vals, topk_idx = torch.topk(sim, k=k, dim=1)\n",
    "        for t in range(text_len):\n",
    "            for j in range(k):\n",
    "                patch_idx = topk_idx[t, j].item()\n",
    "                weight = topk_vals[t, j].item()\n",
    "                t_node = t\n",
    "                p_node = text_len + patch_idx\n",
    "                edge_index.extend([(t_node, p_node), (p_node, t_node)])\n",
    "                edge_type.extend([EDGE_TEXT_IMG, EDGE_TEXT_IMG])\n",
    "                edge_weight.extend([weight, weight])\n",
    "    \n",
    "    # Global connections\n",
    "    global_idx = text_len + img_len\n",
    "    for node in range(global_idx):\n",
    "        edge_index.extend([(global_idx, node), (node, global_idx)])\n",
    "        edge_type.extend([EDGE_GLOBAL, EDGE_GLOBAL])\n",
    "        edge_weight.extend([0.0, 0.0])\n",
    "    \n",
    "    edge_index_tensor = torch.tensor(edge_index, device=device, dtype=torch.long).t()\n",
    "    edge_type_tensor = torch.tensor(edge_type, device=device, dtype=torch.long)\n",
    "    edge_weight_tensor = torch.tensor(edge_weight, device=device, dtype=torch.float)\n",
    "    \n",
    "    text_indices = torch.arange(text_len, device=device, dtype=torch.long)\n",
    "    image_indices = torch.arange(img_len, device=device, dtype=torch.long) + text_len\n",
    "    \n",
    "    return {\n",
    "        \"x\": x.float(),\n",
    "        \"edge_index\": edge_index_tensor,\n",
    "        \"edge_type\": edge_type_tensor,\n",
    "        \"edge_weight\": edge_weight_tensor,\n",
    "        \"text_indices\": text_indices,\n",
    "        \"image_indices\": image_indices,\n",
    "        \"global_index\": torch.tensor(global_idx, device=device, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "\n",
    "class GraphormerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, attn_bias, node_mask):\n",
    "        B, N, _ = x.shape\n",
    "        qkv = self.qkv(x).view(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim**0.5)\n",
    "        scores = scores + attn_bias\n",
    "        \n",
    "        key_mask = node_mask.unsqueeze(1).unsqueeze(2)\n",
    "        scores = scores.masked_fill(~key_mask, float(\"-inf\"))\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, self.d_model)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.dropout(out)\n",
    "        x = self.norm1(x + out)\n",
    "        \n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphormerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=256, num_heads=4, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.node_proj = nn.Linear(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GraphormerLayer(d_model, num_heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.edge_type_emb = nn.Embedding(4, num_heads)\n",
    "        \n",
    "        self.conflict_eps = 1e-6\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model * 3 + 1, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "    \n",
    "    def _build_attention_bias(self, edge_index_list, edge_type_list, edge_weight_list, max_nodes, device):\n",
    "        bias = torch.zeros(len(edge_index_list), self.num_heads, max_nodes, max_nodes, device=device)\n",
    "        et_emb = self.edge_type_emb.weight\n",
    "        \n",
    "        for b, (edge_index, edge_type, edge_weight) in enumerate(zip(edge_index_list, edge_type_list, edge_weight_list)):\n",
    "            if edge_index.numel() == 0:\n",
    "                continue\n",
    "            src = edge_index[0]\n",
    "            dst = edge_index[1]\n",
    "            et = edge_type\n",
    "            ew = edge_weight\n",
    "            bias[b, :, src, dst] += et_emb[et].transpose(0, 1)\n",
    "            \n",
    "            text_img_mask = et == EDGE_TEXT_IMG\n",
    "            if text_img_mask.any():\n",
    "                sim_vals = ew[text_img_mask].unsqueeze(0)\n",
    "                bias[b, :, src[text_img_mask], dst[text_img_mask]] += sim_vals\n",
    "        return bias\n",
    "    \n",
    "    def forward(self, node_feats, node_mask, text_mask, image_mask, global_indices,\n",
    "                edge_index_list, edge_type_list, edge_weight_list):\n",
    "        device = node_feats.device\n",
    "        max_nodes = node_feats.size(1)\n",
    "        \n",
    "        attn_bias = self._build_attention_bias(\n",
    "            edge_index_list, edge_type_list, edge_weight_list, max_nodes, device\n",
    "        )\n",
    "        \n",
    "        x = self.node_proj(node_feats)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_bias, node_mask)\n",
    "        \n",
    "        text_mask_f = text_mask.float()\n",
    "        image_mask_f = image_mask.float()\n",
    "        text_count = text_mask_f.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "        image_count = image_mask_f.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "        \n",
    "        text_pool = (x * text_mask_f.unsqueeze(-1)).sum(dim=1) / text_count\n",
    "        image_pool = (x * image_mask_f.unsqueeze(-1)).sum(dim=1) / image_count\n",
    "        \n",
    "        batch_indices = torch.arange(x.size(0), device=device)\n",
    "        global_embeds = x[batch_indices, global_indices]\n",
    "        \n",
    "        conflict = 1.0 - F.cosine_similarity(text_pool, image_pool, dim=-1, eps=self.conflict_eps)\n",
    "        conflict = conflict.unsqueeze(-1)\n",
    "        \n",
    "        combined = torch.cat([global_embeds, text_pool, image_pool, conflict], dim=-1)\n",
    "        logits = self.mlp(combined).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ Model architecture loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load checkpoint\n",
    "print(\"\\nüì• Loading trained model...\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "cfg = checkpoint[\"config\"]\n",
    "\n",
    "# Load CLIP\n",
    "clip_backbone = cfg.get(\"clip_backbone\", \"openai/clip-vit-base-patch32\")\n",
    "clip_layer_idx = cfg.get(\"clip_layer_idx\", -1)\n",
    "\n",
    "print(f\"Loading CLIP: {clip_backbone}\")\n",
    "processor = CLIPProcessor.from_pretrained(clip_backbone)\n",
    "clip_model = CLIPModel.from_pretrained(clip_backbone).to(device)\n",
    "\n",
    "if \"clip_state\" in checkpoint:\n",
    "    clip_model.load_state_dict(checkpoint[\"clip_state\"])\n",
    "    print(\"‚úì Loaded fine-tuned CLIP\")\n",
    "\n",
    "clip_model.eval()\n",
    "\n",
    "# Projection layers\n",
    "shared_dim = cfg[\"input_dim\"]\n",
    "text_dim = cfg.get(\"text_dim\", shared_dim)\n",
    "vision_dim = cfg.get(\"vision_dim\", 768)\n",
    "\n",
    "text_proj = nn.Identity() if text_dim == shared_dim else nn.Linear(text_dim, shared_dim)\n",
    "image_proj = nn.Linear(vision_dim, shared_dim)\n",
    "text_proj = text_proj.to(device)\n",
    "image_proj = image_proj.to(device)\n",
    "\n",
    "if \"text_proj_state\" in checkpoint:\n",
    "    text_proj.load_state_dict(checkpoint[\"text_proj_state\"])\n",
    "if \"image_proj_state\" in checkpoint:\n",
    "    image_proj.load_state_dict(checkpoint[\"image_proj_state\"])\n",
    "\n",
    "text_proj.eval()\n",
    "image_proj.eval()\n",
    "\n",
    "# Graphormer\n",
    "model = GraphormerModel(\n",
    "    input_dim=shared_dim,\n",
    "    d_model=cfg[\"d_model\"],\n",
    "    num_heads=cfg[\"num_heads\"],\n",
    "    num_layers=cfg[\"num_layers\"],\n",
    "    dropout=cfg.get(\"dropout\", 0.1),\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "top_k = cfg.get(\"top_k\", 4)\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Layers: {cfg['num_layers']}, Dim: {cfg['d_model']}, Top-K: {top_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_meme(image, text):\n",
    "    \"\"\"Predict if a meme is hateful.\"\"\"\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        \n",
    "        text_hidden = outputs.text_model_output.hidden_states[clip_layer_idx]\n",
    "        vision_hidden = outputs.vision_model_output.hidden_states[clip_layer_idx]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        # Build graph\n",
    "        txt_len = int(attention_mask[0].sum().item())\n",
    "        txt_start = 1\n",
    "        txt_end = max(txt_len - 1, txt_start)\n",
    "        text_feats = text_hidden[0, txt_start:txt_end]\n",
    "        text_feats = text_proj(text_feats)\n",
    "        \n",
    "        image_feats = vision_hidden[0, 1:]\n",
    "        image_feats = image_proj(image_feats)\n",
    "        \n",
    "        graph = build_graph(text_feats, image_feats, top_k)\n",
    "        \n",
    "        # Prepare batch\n",
    "        node_feats = graph[\"x\"].unsqueeze(0)\n",
    "        N = node_feats.size(1)\n",
    "        node_mask = torch.ones(1, N, dtype=torch.bool, device=device)\n",
    "        \n",
    "        text_mask = torch.zeros(1, N, dtype=torch.bool, device=device)\n",
    "        text_mask[0, graph[\"text_indices\"]] = True\n",
    "        \n",
    "        image_mask = torch.zeros(1, N, dtype=torch.bool, device=device)\n",
    "        image_mask[0, graph[\"image_indices\"]] = True\n",
    "        \n",
    "        global_indices = graph[\"global_index\"].unsqueeze(0)\n",
    "        \n",
    "        # Predict\n",
    "        logits = model(\n",
    "            node_feats, node_mask, text_mask, image_mask, global_indices,\n",
    "            [graph[\"edge_index\"]], [graph[\"edge_type\"]], [graph[\"edge_weight\"]]\n",
    "        )\n",
    "        \n",
    "        prob = torch.sigmoid(logits).item()\n",
    "    \n",
    "    return prob\n",
    "\n",
    "print(\"‚úÖ Prediction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interactive Demo - Upload & Test!\n",
    "\n",
    "**Upload a meme image and enter its text below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Upload widget\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "text_input = widgets.Textarea(placeholder='Enter meme text here...', description='Meme Text:')\n",
    "button = widgets.Button(description='üîé Analyze Meme', button_style='primary')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    with output_area:\n",
    "        if len(uploader.value) == 0:\n",
    "            print(\"‚ùå Please upload an image first!\")\n",
    "            return\n",
    "        \n",
    "        # Get image\n",
    "        uploaded_file = list(uploader.value.values())[0]\n",
    "        image = Image.open(uploaded_file['content'])\n",
    "        \n",
    "        # Get text\n",
    "        text = text_input.value.strip()\n",
    "        if not text:\n",
    "            text = \"[No text provided]\"\n",
    "        \n",
    "        print(\"üîÑ Processing...\\n\")\n",
    "        \n",
    "        # Predict\n",
    "        prob = predict_meme(image, text)\n",
    "        \n",
    "        # Display results\n",
    "        is_hateful = prob > 0.5\n",
    "        confidence = prob if is_hateful else (1 - prob)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        if is_hateful:\n",
    "            print(\"üö´ HATEFUL CONTENT DETECTED\")\n",
    "            print(\"  ‚ö†Ô∏è  This meme contains offensive content\")\n",
    "        else:\n",
    "            print(\"‚úÖ SAFE CONTENT\")\n",
    "            print(\"  ‚úì This meme is appropriate\")\n",
    "        \n",
    "        print(f\"\\nüìä Statistics:\")\n",
    "        print(f\"   Hateful Probability: {prob:.1%}\")\n",
    "        print(f\"   Confidence: {confidence:.1%}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display image\n",
    "        display(image.resize((300, 300)))\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display interface\n",
    "display(HTML(\"<h2>üì§ Upload Meme & Get Prediction</h2>\"))\n",
    "display(uploader)\n",
    "display(text_input)\n",
    "display(button)\n",
    "display(output_area)\n",
    "\n",
    "print(\"\\n‚ú® Demo ready! Upload an image and click 'Analyze Meme'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
